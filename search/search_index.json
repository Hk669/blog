{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hey there!","text":"<p>I'm Hrushikesh Dokala, and I'm currently a Maintainer at AutoGen, diving deep into AI infrastructure and building applications around AI Agents. I specialize in backend development and love contributing to open source projects. I'm a Beta Microsoft Learn Student Ambassadors.</p> <p>Currently seeking job opportunities. Grateful for any referrals. Thank you in advance.</p>"},{"location":"#packages-i-built","title":"Packages I Built","text":"<p>built my own tokenizer package to train the Tokenizer of LLM.</p> <ul> <li>bpetokenizer, supports special tokens and regex expression. (inspired from tiktoken) you can also read about this on link.</li> </ul>"},{"location":"#writing","title":"Writing","text":"<ul> <li>Atlan Interview Process</li> <li>RAG</li> <li>Large Language Models</li> <li>Docker</li> <li>Domain Name System</li> <li>all you need is pydantic</li> </ul>"},{"location":"#work-history","title":"Work History","text":""},{"location":"#open-source","title":"Open Source","text":"<ul> <li>AutoGen</li> <li>Hack Club</li> <li>MLJAR supervised</li> </ul> <p>For more details about my work, feel free to explore my portfolio site </p> <p>Thanks for stopping by!</p> <p>Just a heads up, everything shared on my site reflects my own insights and perspectives. I'm always open to corrections and feedback.</p>"},{"location":"blog/","title":"\ud83c\udf1f whats up?","text":"<p>i really wanted this to be automated. i want to make use of the voice model to generate the blog while im talking. i think i'll start working on this. </p>"},{"location":"blog/#questions","title":"Questions?","text":"<ul> <li>If you have topics you'd like me to write about leave a comment in my discussions</li> </ul>"},{"location":"blog/#writing","title":"Writing","text":"<ul> <li>Domain Name System</li> <li>all you need is pydantic</li> </ul>"},{"location":"writing/","title":"whats up?","text":"<p>i really wanted this to be automated. i want to make use of the voice model to generate the blog while im talking. i think i'll start working on this. </p>"},{"location":"writing/#questions","title":"Questions?","text":"<ul> <li>If you have topics you'd like me to write about leave a comment in my discussions</li> </ul>"},{"location":"writing/2024/11/09/atlan-interview-process/","title":"Atlan","text":""},{"location":"writing/2024/11/09/atlan-interview-process/#my-journey-to-atlan-an-interview-experience","title":"My Journey to Atlan: An Interview Experience","text":"<p>this is post the OA and the screening round.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#take-home-assignment","title":"Take Home Assignment","text":"<p>The unique hiring process of the Atlan, validates its candidates on the technical expertise and understanding of the systems and not just the DSA like FAANG. I was given the task of designing and building a highly scalable logistics platform for goods transportation. This platform would allow users to book transportation services, connecting them with a fleet of drivers and offering real-time availability, pricing, and vehicle tracking, over a week of time.</p> <p>After a week of submission recieved a mail, we loved your solution and moving forward with your applicaiton.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#technical-interview","title":"Technical Interview","text":"<p>The interview began with a senior software engineer who introduced himself and explained that another interviewer would join us in about 15 minutes. After our introductions, we dove straight into the high-level design (HLD) discussion of the take home assignment.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#key-questions-and-challenges","title":"Key Questions and Challenges","text":"<p>During the HLD, I encountered a few questions that required careful thought:</p> <ul> <li> <p>Why both a load balancer and an API gateway?</p> </li> <li> <p>Using Redis, a key-value store, as a queue \u2013 how does that work?</p> </li> </ul> <p>On a positive note, I was able to answer some questions well:</p> <ul> <li> <p>Why not use WebSockets for two-way communication in this case?</p> </li> <li> <p>I explained that, for this specific application, polling would be more efficient than WebSockets.</p> </li> </ul> <p>After around 20 minutes discussing the HLD, we moved on to a coding question: find the largest palindromic number that can be achieved by the product of two 3-digit numbers.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#coding-question","title":"Coding Question","text":"<p>I started by explaining a brute-force approach and began coding. However, I initially stumbled when structuring the code into a function and accidentally forgot to join the reversed object after converting it to a list. Fortunately, the interviewers helped me debug, and I managed to complete the solution.</p> <p>The last 15 minutes involved discussing:</p> <ul> <li> <p>Data governance and internal processes</p> </li> <li> <p>AI integrations they were building to boost productivity and improve client interactions</p> </li> </ul> <p>I also shared my experience with my marketing agent workflow, which seemed to resonate well with them. The conversation wrapped up on a positive note, and I sent a thank-you email to the recruiter afterward. Now, I'm awaiting the results and hoping for a call for the next round\u2014fingers crossed.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#cultural-round","title":"Cultural Round","text":"<p>The cultural round kicked off with introductions. The interviewer shared his background, including his role, college, and journey. He asked me to share my story, starting from childhood, and to reflect on how my journey has shaped me.</p> <p>We discussed my college experiences, remote working with AutoGen, VEnable AI, and ForgeWize, and my ability to work independently. Things were going well, although, midway, I sensed a slight shift in his interest as he checked his phone occasionally, which felt a bit off-putting.</p> <p>One insightful question he asked was:</p> <ul> <li>If I were to call the founders of the startups you worked with previously, what\u2019s one pro and one con they might mention about you?</li> </ul> <p>When he invited me to ask questions, I did, and we concluded the meeting. Despite the minor interruptions, I felt the cultural round went well. I\u2019m hopeful for a positive outcome but know that whatever happens, this has been a valuable learning experience.</p>"},{"location":"writing/2024/11/09/atlan-interview-process/#the-result","title":"The Result","text":"<p>I finally received a call from the recruiter: I got selected! I\u2019ll be heading to Delhi next month for onboarding week. This experience has been a rollercoaster. I was feeling quite down, seeing my peers from college landing MNC roles, but I\u2019m thrilled to be joining Atlan. This journey taught me resilience and the importance of perseverance in the face of setbacks.</p>"},{"location":"writing/2024/07/14/bpetokenizer/","title":"Bpetokenizer","text":"<p>A Byte Pair Encoding (BPE) tokenizer, which algorithmically follows along the GPT tokenizer(tiktoken), allows you to train your own tokenizer. The tokenizer is capable of handling special tokens and uses a customizable regex pattern for tokenization(includes the gpt4 regex pattern). supports <code>save</code> and <code>load</code> tokenizers in the <code>json</code> format. The <code>bpetokenizer</code> also supports pretrained tokenizers.</p>"},{"location":"writing/2024/07/14/bpetokenizer/#overview","title":"Overview","text":"<p>The Byte Pair Encoding (BPE) algorithm is a simple yet powerful method for building a vocabulary of subword units for a given text corpus. This tokenizer can be used for training your tokenizer of the LLM on various languages of text corpus.</p> <p>this algorithm is first introduced in the paper Neural Machine Translation of Rare Words with Subword Units and then used this in the gpt2 tokenizer(Language Models are Unsupervised Multitask Learners)</p> <p>Every LLM(LLama, Gemini, Mistral..) use their own Tokenizers trained on their own text dataset.</p>"},{"location":"writing/2024/07/14/bpetokenizer/#features","title":"Features","text":"<ul> <li>Implements Byte Pair Encoding (BPE) algorithm.</li> <li>Handles special tokens.</li> <li>Uses a customizable regex pattern for tokenization.</li> <li>Compatible with Python 3.9 and above</li> </ul>"},{"location":"writing/2024/07/14/bpetokenizer/#this-repository-has-3-different-tokenizers","title":"This repository has 3 different Tokenizers:","text":"<ul> <li><code>BPETokenizer</code></li> <li><code>Tokenizer</code></li> <li> <p><code>PreTrained</code></p> </li> <li> <p>Tokenizer: This class contains <code>train</code>, <code>encode</code>, <code>decode</code> and functionalities to <code>save</code> and <code>load</code>. Also contains few helper functions <code>get_stats</code>, <code>merge</code>, <code>replace_control_characters</code>..  to perform the BPE algorithm for the tokenizer.</p> </li> <li> <p>BPETokenizer: This class emphasizes the real power of the tokenizer(used in gpt4 tokenizer..tiktoken), uses the <code>GPT4_SPLIT_PATTERN</code> to split the text as mentioned in the gpt4 tokenizer. also handles the <code>special_tokens</code>. which inherits the <code>save</code> and <code>load</code> functionlities to save and load the tokenizer respectively.</p> </li> <li> <p>PreTrained Tokenizer: PreTrained Tokenizer wi17k_base, has a 17316 vocabulary. trained with the wikitext dataset (len: 1000000). with 6 special_tokens.</p> </li> </ul>"},{"location":"writing/2024/07/14/bpetokenizer/#usage","title":"Usage","text":"<p>this tutorial leverages the <code>special_tokens</code> usage in the Tokenizer.</p> <p>Install the package</p> <pre><code>pip install bpetokenizer\n</code></pre> <pre><code>from bpetokenizer import BPETokenizer\n\nspecial_tokens = {\n    \"&lt;|endoftext|&gt;\": 1001,\n    \"&lt;|startoftext|&gt;\": 1002,\n    \"[SPECIAL1]\": 1003,\n    \"[SPECIAL2]\": 1004,\n}\n\ntokenizer = BPETokenizer(special_tokens=special_tokens) # you can also use the method _special_tokens to register the special tokens (if not passed when intializing)\ntexts = \"&lt;|startoftext|&gt; Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.&lt;|endoftext|&gt;\"\n\ntokenizer.train(texts, vocab_size=310, verbose=True)\n# tokenizer._special_tokens(special_tokens) # if not passed when intialization of the BPETokenizer\n\nencode_text = \"\"\"\n&lt;|startoftext|&gt;Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\nHello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\nGreetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.\nHello, World! This is yet another sample text, with [SPECIAL1] and [SPECIAL2] making an appearance.\nHey there, World! Testing the tokenizer with [SPECIAL1] and [SPECIAL2] to see if it handles special tokens properly.\nSalutations, Planet! The tokenizer should recognize [SPECIAL1] and [SPECIAL2] in this long string of text.\nHello again, World! [SPECIAL1] and [SPECIAL2] are special tokens that need to be handled correctly by the tokenizer.\nWelcome, World! Including [SPECIAL1] and [SPECIAL2] multiple times in this large text to ensure proper encoding.\nHi, World! Let's add [SPECIAL1] and [SPECIAL2] in various parts of this long sentence to test the tokenizer thoroughly.\n&lt;|endoftext|&gt;\n\"\"\"\nids = tokenizer.encode(encode_text, special_tokens=\"all\")\nprint(ids)\n\ndecode_text = tokenizer.decode(ids)\nprint(decode_text)\n\ntokenizer.save(\"sample_bpetokenizer\", mode=\"json\")\n</code></pre>"},{"location":"writing/2024/07/14/bpetokenizer/#to-load-the-tokenizer","title":"To Load the Tokenizer","text":"<p><pre><code>from bpetokenizer import BPETokenizer\n\ntokenizer = BPETokenizer()\n\ntokenizer.load(\"sample_bpetokenizer.json\", mode=\"json\")\n\nencode_text = \"\"\"\n&lt;|startoftext|&gt;Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\nHello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\nGreetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.&lt;|endoftext|&gt;\"\"\"\n\nprint(\"vocab: \", tokenizer.vocab)\nprint('---')\nprint(\"merges: \", tokenizer.merges)\nprint('---')\nprint(\"special tokens: \", tokenizer.special_tokens)\n\nids = tokenizer.encode(encode_text, special_tokens=\"all\")\nprint('---')\nprint(ids)\n\ndecode_text = tokenizer.decode(ids)\nprint('---')\nprint(decode_text)\n\n# you can also print the tokens and the text chunks split with the pattern.\ntokens = tokenizer.tokens(encode_text, verbose=True) # if verbose, prints the text chunks and also the pattern used to split.\nprint('---')\nprint(\"tokens: \", tokens)\n</code></pre> refer to the load_json_vocab and run the <code>bpetokenizer_json</code> to get an overview of <code>vocab</code>, <code>merges</code>, <code>special_tokens</code> and to view the tokens that are split by the tokenizer using pattern, look at tokens</p>"},{"location":"writing/2024/07/14/bpetokenizer/#to-load-the-pretrained-tokenizers","title":"To load the pretrained tokenizers","text":"<p><pre><code>from bpetokenizer import BPETokenzier\n\ntokenizer = BPETokenizer.from_pretrained(\"wi17k_base\", verbose=True)\n\ntexts = \"\"\"\ndef get_stats(tokens, counts=None) -&gt; dict:\n    \"Get statistics of the tokens. Includes the frequency of each consecutive pair of tokens\"\n    counts = if counts is None else counts\n    for pair in zip(tokens, tokens[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\"\"\"\ntokenizer.tokens(texts, verbose=True)\n</code></pre> for now, we only have a single 17k vocab tokenizer <code>wi17_base</code> at pretrained</p>"},{"location":"writing/2024/07/14/bpetokenizer/#run-tests","title":"Run Tests","text":"<p>the tests folder <code>tests/</code> include the tests of the tokenizer, uses pytest.</p> <pre><code>python3 -m pytest\n</code></pre> <p>additionally, the workflows are setup to run the tests when made a PR.</p> <p>Thanks for reading the blog! I believe you got an understanding on how the tokenizers power and a crucial part of the LLMs.</p>"},{"location":"writing/2024/01/30/domain-name-system/","title":"Domain Name System","text":"<p>I think the basic to know before starting the web development, is to know how the web actually works.</p> <p>You type \u201cwww.google.com\u201d into your browser, and like magic, the Google homepage appears. But have you ever wondered  how your computer knows where to find Google in the vast ocean of the internet? Enter the unsung hero of the online world \u2014 the  Domain Name System (DNS) server. Let\u2019s embark on a journey to demystify DNS, uncovering the behind-the-scenes ballet that brings you  seamlessly to your favorite websites.</p> <p></p>"},{"location":"writing/2024/01/30/domain-name-system/#understanding-the-basics","title":"Understanding the Basics","text":"<p>Think of a domain like a friend\u2019s address \u2014 easy for you to remember. However, computers speak in a different language, using  numerical IP addresses. When you type a domain like <code>www.google.com</code>, your computer turns to the DNS server, asking, \u201cWhere\u2019s Google  at?\u201d The DNS server then translates the human-friendly domain into a machine-readable IP address, like <code>192.0.2.44</code>.</p>"},{"location":"writing/2024/01/30/domain-name-system/#the-translation-process","title":"The Translation Process","text":"<p>It\u2019s like having a personal translator for the internet. Your computer sends a request to the DNS server, saying, \u201cI need Google!\u201d  The server responds, \u201cGoogle? Sure, that\u2019s at <code>192.0.2.44</code>.\u201d Now armed with the correct address, your computer connects to the Google  server, and voila, the webpage appears on your screen.</p> <p>if we go more deeper to understand the process, here is a high level architecture which brings you the Google</p> <p>the DNS resolver\u2019s first step is to find the TLD (top level domain). which basically stores all the domains with the same extension  (.com, .in,\u2026) \u2014 once it knows the TLD, either .com or any other , it searches for the right IP address.</p> <p>Once the TLD is conquered, our DNS resolver hero hunts down the right IP address, the golden key to the website\u2019s location.  Triumphantly, it hands this precious information to the Authoritative Server \u2014 the wise sage who holds the secrets to the specific  domain\u2019s whereabouts in the vast internet realm.</p> <p>The grand finale! The DNS resolver sends a request to the verified IP address. If the IP responds with the exact data needed, it\u2019s a  match made in cyberspace. With a satisfied nod, the DNS resolver hands over the correct IP to your web browser, conjuring the  requested webpage onto your screen like magic.</p> <p>In the symphony of the internet, DNS servers are the composers ensuring harmony between human-friendly domains and computer-friendly  IP addresses. They\u2019re the navigators steering your requests through the web\u2019s intricate pathways, making sure you reach your desired  destination without a hitch. Next time you effortlessly surf the internet, remember the invisible hand of DNS servers, making the  magic happen behind the scenes.</p> <p>Now, here\u2019s where the magic gets even more interesting. To avoid the tedious process of searching for the same information every  time, DNS servers employ a clever trick called caching. Once the DNS resolver finds the correct IP address, it stores this  information in a cache \u2014 a sort of digital memory. So, the next time you make the same request, the DNS server simply retrieves the  information from its cache, skipping the elaborate journey and speeding up the entire process.</p> <p>To speed things up, DNS servers remember recent translations. The first time you visit a site, the DNS server does the hard work.  But for subsequent visits, it already knows the way, fetching the information from its memory cache. This nifty trick reduces the  time it takes to load your favorite sites</p> <p>I appreciate you joining me on this adventure. If you have any questions, share your thoughts, or want to dive deeper into  development and foundational domain knowledge, feel free to drop a comment below. Your feedback is invaluable!</p> <p>Happy coding!</p>"},{"location":"writing/2024/02/11/why-docker/","title":"Docker \ud83d\udc33","text":"<p>In this blog, we'll delve into why Docker is a crucial tool for modern software development and how to effectively utilize it. We'll cover Docker from the basics to advanced usage scenarios, exploring its benefits and practical applications.</p>"},{"location":"writing/2024/02/11/why-docker/#problem-statement","title":"Problem Statement","text":"<p>Consider a common scenario: a developer is working on a Django application on their local machine. To run the application successfully, certain requirements and dependencies need to be installed, such as:</p> <pre><code>Django 4.0\nPython 3.1.0\n</code></pre> dev meme <p>dev2: its not working? </p> <p>dev1: but it works on my computer</p> <p>The application runs smoothly on the developer's local environment. However, as time passes, the developer pushes the application code into a remote repository. Months or years later, another developer wants to clone the repository and use the application. Here lies the main problem:</p> <p></p>"},{"location":"writing/2024/02/11/why-docker/#dependency-management-and-environment-consistency","title":"Dependency Management and Environment Consistency:","text":"<p>The new developer may not know all the dependencies required to run the application.</p> <p>Even if they manage to gather the dependencies, there's no guarantee that their local environment matches the specifications of the original developer's environment.</p> <p>Any discrepancies in dependencies or environment configurations can lead to runtime errors and inconsistencies, making it challenging to run the application smoothly.</p> <p>so we use docker to build a container. which will carry :</p> <ul> <li>Operating system (runtime environment)</li> <li>tools</li> <li>libraries and dependencies</li> </ul> <p>if there are multiple developers or a team working on the same application, but everyone has a different config, the container can be copied and used on thier machine.</p> <p>note</p> <p>running a container will not affect the installed dependencies on local machine of the developers. Each container operates in isolation, ensuring that changes made within the container do not impact the host system.</p> <p></p>"},{"location":"writing/2024/02/11/why-docker/#benefits-of-docker","title":"Benefits of Docker","text":"<p>Isolation: Docker containers provide a high level of isolation, ensuring that applications run predictably regardless of differences in the host environment.</p> <p>Portability: Docker containers are portable and can be easily moved between different environments, streamlining the deployment process.</p> <p>Efficiency: Docker uses a layered filesystem and smart caching mechanisms, resulting in faster build times and reduced storage overhead compared to traditional virtual machines.</p> <p>By leveraging Docker, developers can simplify dependency management, ensure environment consistency, and streamline the deployment process for their applications.</p>"},{"location":"writing/2024/02/11/why-docker/#components-of-docker","title":"Components of Docker","text":"<p>Docker Engine: It is the core component of docker, which is lightweight runtime and orchestrator which manages the containers, runs their lifecycle and provides networking and storage capabilities.</p> <p></p> <p>Note</p> <p>Docker daemon (<code>dockerd</code>) which runs on the host machine and the Docker client, a cli to interact with Docker daemon</p> <p>Docker Image: where the code is stored or the runtime envs are stored. Image is a lightweight, standalone and executable package which stores all the necessary files required to run the application.</p> <p></p> Note <ul> <li>Immutable</li> <li>Layered File system</li> <li>Stored in Registry</li> </ul> <p>Docker Container: It is a runnable instance of the Image we built for an application. which encapsulates all the necessary files, dependencies required to run the application</p> Note <p>containers have a life cycle, which means:</p> <ul> <li>Start</li> <li>Stop</li> <li>Delete</li> </ul> <p>DockerFile: the maker of the Image, a file with set of instructions on how to build the Image. Contains dependencies, base image, env variables to build an image</p> <ul> <li>To build an Image we use <code>docker build</code></li> </ul> <p>Docker Registry: Docker registry is a repository for storing and sharing Docker images. It serves as a centralized location where developers can push and pull images, making them accessible to other users and systems.</p> Tip <p>Docker Registry is similar to GitHub.</p> <p>checkout my Docker Hub, Built an Image of the documentaion site, can pull and contribute anyone who's interested.</p> <p>Docker Compose: Docker Compose is a tool for defining and running multi-container Docker applications. It uses a YAML file (docker-compose.yml) to define the services, networks, and volumes needed for an application.</p> <ul> <li>Docker Compose is an advanced topic we'll cover it when we dive deep into the orchestration.</li> </ul>"},{"location":"writing/2024/02/11/why-docker/#conclusion","title":"Conclusion","text":"<p>I think its a good start to understand what is Docker and why we use it. As a developer understanding the stuff like Containers and isolation will help you in the long journey. </p> <p>Thank you for your attention and interest until now. will soon drop a video showing the live interaction with all the components for better understanding.</p> Announcement <p>I dropped a documentation for my DSA solutions of leetcode problems. please feel free to checkout the site. link</p>"},{"location":"writing/2024/07/18/eb-for-fastapi/","title":"Deploy FastAPI using AWS Elastic BeanStalk","text":"<p>i wont get into the development part of the application using FastAPI, this blog will be completely focused on the deployment and operations part of the application.</p>"},{"location":"writing/2024/07/18/eb-for-fastapi/#install-the-dependencies","title":"Install the dependencies","text":"Tip <p>Be sure to register an IAM user for getting access to your AWS account.</p> <p>To access the Elastic Beanstalk in your laptop, install the EB CLI or with pip</p> Tip <p>Do check the colorama and other dependencies if you are using the pip awsebcli. it can break your entire environment.</p> <p>After installing, check if the EB CLI is installed perfectly by running:</p> <pre><code>eb --version\n</code></pre> <p>Once everything mentioned above is setup:</p>"},{"location":"writing/2024/07/18/eb-for-fastapi/#intialize-the-project","title":"Intialize the project","text":"<pre><code>eb init\n</code></pre> <p>You'll be asked multiple questions:</p> <ul> <li>Default Region</li> <li>Applicaiton Name</li> <li>Platform</li> <li>Asks for the Code Commit, be sure if you are using the git or code commit to push changes in your applicaiton</li> </ul> <p>After answering all the questions, it adds a new directory in the project dir <code>.elasticbeanstalk</code>.</p> <pre><code>.elasticbeanstalk\n----- config.yml\n</code></pre> <p>The file should be something more like this:</p> <pre><code># config.yml\n\nbranch-defaults:\n  branch-name:\n    environment: Env-dev\n    group_suffix: null\nglobal:\n  application_name: App Name\n  branch: null\n  default_ec2_keyname: key-pair\n  default_platform: Python 3.11 running on 64bit Amazon Linux 2023\n  default_region: ap-south-1\n  include_git_submodules: true\n  instance_profile: null\n  platform_name: null\n  platform_version: null\n  profile: null\n  repository: null\n  sc: git\n  workspace_type: Application\n</code></pre>"},{"location":"writing/2024/07/18/eb-for-fastapi/#create-the-environment","title":"Create the Environment","text":"<pre><code>eb create\n</code></pre> <p>Again, you'll be prompted with a few questions.</p> <p>Once you are done with the questions, your environment will spin up and the Elastic BeanStalk will spin up EC2 instance with your FastAPI application.</p> <p>you can view the status of the environment by:</p> <pre><code>eb status\n</code></pre> <pre><code>Environment details for: Env-Name\n  Application name: Application Name\n  Region: ap-south-1\n  Deployed Version: app-82fb-2203743843434324\n  Environment ID: e-nsizyek74z\n  Platform: arn:aws:elasticbeanstalk:us-west-2::platform/Python 3.8 running on 64bit Amazon Linux 2/3.3.11\n  Tier: WebServer-Standard-1.0\n  CNAME: cname-env.us-west-2.elasticbeanstalk.com\n  Updated: 2024-07-11 23:16:03.822000+00:00\n  Status: Launching\n  Health: Red\n</code></pre>"},{"location":"writing/2024/07/18/eb-for-fastapi/#configure-an-environment","title":"Configure an Environment","text":"<p>In the previous step, we tried accessing our application and it returned 502 Bad Gateway. There are three reasons behind it:</p> <p>Python needs <code>PYTHONPATH</code> in order to find modules in our application. By default, Elastic Beanstalk attempts to launch the WSGI application from application.py, which doesn't exist. If not specified otherwise, Elastic Beanstalk tries to serve Python applications with Gunicorn. </p> <p>Gunicorn by itself is not compatible with FastAPI since FastAPI uses the newest ASGI standard.</p> <p>Let's fix these errors.</p> <p>Create a new folder in the project root called <code>.ebextensions</code>. Within the newly created folder create a file named <code>01_fastapi.config</code>:</p> <pre><code># .ebextensions/01_fastapi.config\n\noption_settings:\n  aws:elasticbeanstalk:application:environment:\n    PYTHONPATH: \"/var/app/current:$PYTHONPATH\"\n  aws:elasticbeanstalk:container:python:\n    WSGIPath: \"main:app\"\n\ncontainer_commands:\n    01_init:\n        command: \"source /var/app/venv/*/bin/activate &amp;&amp; pip install -r requirements.txt\"\n</code></pre> <p>The important part for running the application on the environment:</p>"},{"location":"writing/2024/07/18/eb-for-fastapi/#create-a-procfile","title":"Create a Procfile","text":"<p>make a file with name <code>Procfile</code> in the project directory</p> <pre><code>web: gunicorn main:app --workers=4 --worker-class=uvicorn.workers.UvicornWorker\n</code></pre> <p>At this stage after completing all the above steps, the project structure should look like this:</p> <pre><code>|-- .ebextensions\n|   \u2514-- 01_fastapi.config\n|-- .elasticbeanstalk\n|   \u2514-- config.yml\n|-- .gitignore\n|-- Procfile\n|-- README.md\n|-- main.py\n|-- models.py\n`-- requirements.txt\n</code></pre> <p>You can add in the ENV variables in the <code>Configuration</code> tab of the environment.</p> <p>Now commit the changes to your git repository and deploy:</p> <pre><code>git add .\n\ngit commit -m \"feat: elastic beanstalk added\"\n\ngit push \n\neb deploy\n</code></pre> <p>After the deployment you can view the status of the application from the AWS portal or you can use:</p> <pre><code>eb status\n</code></pre> <p>For getting a more detailed logs:</p> <pre><code>eb logs\n</code></pre>"},{"location":"writing/2024/07/18/eb-for-fastapi/#terminate","title":"Terminate","text":"<p>If you are done with testing and want to remove all the resources associated with this project:</p> <pre><code>eb terminate\n</code></pre>"},{"location":"writing/2024/07/18/eb-for-fastapi/#my-checklist","title":"My CheckList","text":"<p>I use AWS Amplify(service for deploying static sites) for my client and AWS EB for the server deployment.</p>"},{"location":"writing/2024/07/18/eb-for-fastapi/#client","title":"Client","text":"<ul> <li> Update the server uri in the build</li> <li> Also remove all the console statements to not disclose any information</li> <li> Add the SSL Certificate to the client</li> </ul>"},{"location":"writing/2024/07/18/eb-for-fastapi/#server","title":"Server","text":"<ul> <li> Update the Security Rules for the environment (Inbound and Outbound)</li> <li> Update the client uri in the ENV Variables and other if required</li> <li> If using any Auth providers, append your server uri</li> <li> If using HTTPS, add the <code>Listener</code> for port 443 and select the SSL certificate from ACM</li> </ul>"},{"location":"writing/2024/07/25/gitmatch/","title":"Gitmatch","text":""},{"location":"writing/2024/07/25/gitmatch/#discover-the-best-open-source-projects-with-gitmatch","title":"Discover the Best Open-Source Projects with GitMatch","text":"<p>Are you a developer seeking to dive into the world of open-source projects but overwhelmed by the countless options available? Look no further! GitMatch is here to help you discover the best projects tailored to your interests and skills.</p>"},{"location":"writing/2024/07/25/gitmatch/#why-use-gitmatch","title":"Why Use GitMatch?","text":"<p>Open-source projects offer a fantastic opportunity to learn new skills, contribute to the community, and showcase your talents to potential employers. However, finding projects that align with your interests and expertise can be challenging. GitMatch simplifies this process by providing personalized project suggestions based on your preferences.</p>"},{"location":"writing/2024/07/25/gitmatch/#key-features","title":"Key Features","text":""},{"location":"writing/2024/07/25/gitmatch/#1-quick-recommendations","title":"1. Quick Recommendations","text":"<p>GitMatch offers a straightforward way to get started with open-source projects. With just one click on the <code>Get Recommendations</code> button, you\u2019ll receive a list of top projects from GitHub. This feature is perfect for those who want to jump straight into exploring without any additional setup.</p>"},{"location":"writing/2024/07/25/gitmatch/#2-personalized-suggestions","title":"2. Personalized Suggestions","text":"<p>For those who want recommendations more closely aligned with their interests, GitMatch allows you to customize your preferences:</p> <ul> <li>Preferred Languages: Enter the programming languages you want the recommendations to focus on. Simply type and hit enter to add each language.</li> <li>Topics: Add topics that interest you, such as <code>AI</code>, <code>machine learning</code>, or <code>web development</code>. These topics help refine the recommendations to match your specific areas of interest.</li> </ul>"},{"location":"writing/2024/07/25/gitmatch/#how-to-get-started","title":"How to Get Started","text":"<ol> <li> <p>Visit GitMatch.in: Head over to our website to start exploring open-source projects tailored just for you.</p> </li> <li> <p>Enter Your Preferences: Customize your recommendations by entering your preferred languages and topics.</p> </li> <li> <p>Get Your Recommendations: Click the <code>Get Recommendations</code> button to receive a curated list of projects that match your interests.</p> </li> </ol>"},{"location":"writing/2024/07/25/gitmatch/#join-the-open-source-community","title":"Join the Open-Source Community","text":"<p>With GitMatch, you can discover projects that not only enhance your skills but also allow you to contribute meaningfully to the open-source community. Whether you\u2019re a beginner or an experienced developer, there\u2019s something for everyone.</p> <p>Start your open-source journey today with GitMatch and unlock a world of opportunities!</p>"},{"location":"writing/2024/02/17/large-language-models/","title":"Lets talk about LLMs","text":""},{"location":"writing/2024/02/17/large-language-models/#my-experience-with-llms","title":"My experience with LLMs","text":"<p>I've explored a variety of Large Language Model(LLMs), ranging from the commercial GPT-4, gemini-pro to open source  alternatives like Mistral-7B and GPT-3.5-turbo. Additionally, I've also explored the smaller language models, like phi-2 of Microsoft which is also an open source model  which is trained on just 2.7B params.</p> <p>Amongst these, gpt4 stands out for its efficiency and the response generation. as per my knowledge, I think there's no llm  that has matched the quality of GPT-4 responses but the information is limited to 2022.</p> <p>I recommend using the gemini-pro for the more updated informtaion and faster response compared to GPT-4.</p>"},{"location":"writing/2024/02/17/large-language-models/#what-are-llms","title":"What are LLMs","text":"<p>Large Language Models, are AI models trained on large text datasets which include articles, books, and other texts.</p> <p>They utilize the transformer architecture, to understand and generate text. These models can perform a wide range of   language-related tasks, including language translation, text summarization, question answering, and natural language   understanding.</p> <p></p> <p>i understand, the architecture is too overwhelming. we'll get a detailed overview on the transformer architecture in the  next blog. check out the paper for more details.</p>"},{"location":"writing/2024/02/17/large-language-models/#what-exactly-the-llms-do","title":"What exactly the LLMs do","text":"<p>Large Language Models excel in generating text that is coherent, contextually relevant, and grammatically correct for an  user's query. They can also perform a wide range of language-related tasks, including language translation, text  summarization, question answering, and natural language understanding.</p> <p>I'll give a small overview on how internal architecture perform the generation and predicting the next word. LLMs consist of two components</p>"},{"location":"writing/2024/02/17/large-language-models/#encoder","title":"Encoder","text":"<p>the encoder is responsible for processing the input text and converting it into a high-dimensional(embeddings) representation that captures its semantic meaning and contextual information.</p> <p>the encoding phase, the LLM processes the input text token by token, generating contextualized embeddings for each token.   These embeddings contain rich information about the input text and are used as input for the subsequent decoding phase.</p> <p></p>"},{"location":"writing/2024/02/17/large-language-models/#decoder","title":"Decoder","text":"<p>The decoder component in LLMs is responsible for generating coherent and contextually relevant responses or continuations  of the sentences based on the encoded input text.</p> <p>While LLMs are primarily trained for autoregressive language generation tasks (i.e., predicting the next token in a  sequence given the preceding context), the decoding process is more complex than in traditional sequence-to-sequence  models. LLMs often employ sophisticated decoding strategies, such as beam search or nucleus sampling, to generate diverse  and fluent responses. </p> info <p>Beam search is a heuristic search algorithm used to find the most likely sequence of tokens in a probabilistic model.</p> <p>Nucleus sampling, also known as top-p sampling or softmax sampling with a dynamic threshold, is a probabilistic  sampling technique used to generate diverse and fluent sequences.</p>"},{"location":"writing/2024/02/17/large-language-models/#conclusion","title":"Conclusion","text":"<p>In conclusion, Large Language Models (LLMs) represent a significant advancement in artificial intelligence, offering remarkable capabilities in understanding and generating human-like text.</p> <p>While each LLM may have its own strengths and limitations, but the final goal is to generate correct, contextual and relevant information for the user query</p> <p>The main problem that is faced is the biasness and the hallucination of the information. they pretend to know  the information but they don't. same as humans, when they dont know the answer, they fabricate the responses and believe that the  answer is true leading to misinformation and inaccuracies. </p> <p>One promising solution to mitigate hallucination is Retrieval Augmented Generation.  By incorporating retrieval-based methods into the generation process, LLMs can access external knowledge sources to enhance the accuracy and relevance of their responses.</p>"},{"location":"writing/2024/01/30/all-you-need-is-pydantic/","title":"all you need is pydantic","text":"<p>Hey there, language enthusiasts! Ever wondered about the dynamic duo of Pydantic and the OpenAI instructor library? Well, you\u2019re in for a treat. This blog is your ticket to exploring how these two pals can tag-team to make your language model interactions not just effective but downright awesome. Join me as we uncover the magic of combining Pydantic\u2019s finesse with the OpenAI instructor library\u2019s wizardry for a seamless and efficient NLP journey.</p>"},{"location":"writing/2024/01/30/all-you-need-is-pydantic/#purpose","title":"Purpose","text":"<p>Why are we diving into this combo, you ask?</p> <p>Simple. Pydantic and the OpenAI instructor library aren\u2019t just tools; they\u2019re superheroes in the world of language processing. Together, they form a powerhouse that not only prompts models like a champ but also ensures the responses are top-notch and well-behaved. This blog? It\u2019s your guide to making this dynamic duo work wonders. Perfect for developers and language lovers who want to make their NLP game strong!</p>"},{"location":"writing/2024/01/30/all-you-need-is-pydantic/#brief","title":"Brief","text":"<p>Before we jump into the fun stuff, let me give you the lowdown on Pydantic and the instructor library.</p> <p>Pydantic: Think of Pydantic as your data sidekick. It\u2019s this cool open-source library that makes sure your data plays nice and stays organized. No more messy data drama; Pydantic\u2019s got it covered.</p> <p>Instructor : Now, the instructor library is like the Swiss Army knife for talking to language models. It taps into OpenAI\u2019s API and makes chatting with models a breeze. What\u2019s cool? You use the same tool to ask questions and get answers. Talk about a smooth operator!</p> <p><pre><code>from instructor import patch\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = patch(OpenAI())\n\nclass ScriptModel(BaseModel):\n    title: str\n    scrpt: int\n\nscript = \"Once upon a time...\"\ntitle = \"A Story\"\n\nprompt = f\"Script: {script}\\n Title: {title}\"\n\nscript= client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=ScriptModel,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Write a story for the given script\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n)\nprint(script)\n</code></pre> the output is now validated by the pydantic, which passes the response through the model and returns the script:</p> <pre><code>{\n    'script': 'Once upon a time...',\n    'title': 'A Story',\n    'choices': [\n        {\n            'message': {\n                'role': 'assistant',\n                'content': 'In a mystical land far, far away, there was a captivating tale waiting to be told. The script unfolded with a magical atmosphere, and the story was aptly named \"A Story.\" As the plot thickened, the characters embarked on a journey filled with twists and turns, ultimately leading to a surprising and heartwarming conclusion.'\n            },\n            'finish_reason': 'stop',\n            'index': 0\n        }\n    ]\n}\n</code></pre>"},{"location":"writing/2024/01/30/all-you-need-is-pydantic/#game","title":"Game","text":"<p>now wonder how can we change the response directly from the llms, just extract the information from the llm and validate through the pydantic model using OpenAISchema from instructor.</p> <pre><code>from instructor import patch, OpenAISchema\nfrom pydantic import Field\nfrom openai import OpenAI\n\nclient = patch(OpenAI())\n\nclass ScreenSegment(OpenAISchema):\n  timestamp:str = Field(..., description='Timestamp of the segment')\n  script:str = Field(..., description='Script of the segment')\n\nclass ScriptModel(OpenAISchema):\n  title : str\n  scene: List[ScreenSegment] = Field(..., description='List of scenes with timestamp')\n\n\nprompt = f\"Script: {script}\\n Title: {title}\"\n\nscript= client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=ScriptModel,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Write a story for the given script\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n)\nprint(script)\n</code></pre> <p>Hold onto your hats! When we run our script, we get a response that\u2019s more than just words. Here\u2019s the breakdown:</p> <pre><code>{\n    'title': 'The Enchanted Forest',\n    'scene': [\n        {\n            'timestamp': '00:01:00',\n            'script': 'Once upon a time, in a magical land, there was an enchanted forest filled with mystical creatures and ancient trees.'\n        },\n        {\n            'timestamp': '00:05:30',\n            'script': 'The sunlight filtered through the leaves, casting a warm glow on the forest floor as the creatures went about their daily activities.'\n        },\n        {\n            'timestamp': '00:10:15',\n            'script': 'One day, a brave adventurer entered the forest, seeking the fabled Fountain of Wishes rumored to grant the deepest desires of those who find it.'\n        }\n        # ... additional segments based on the generated story\n    ]\n}\n</code></pre> <p>title: yep, that\u2019s the title of our story, just as we set it in the user prompt.</p> <p>scene: imagine a list of carefully crafted scenes, each with a timestamp and a script.</p> <p>It\u2019s like a well-organized party of storytelling goodness. This format? Makes it a breeze to take the model\u2019s creativity and plug it into different applications.</p>"},{"location":"writing/2024/01/30/all-you-need-is-pydantic/#conclusion","title":"Conclusion","text":"<p>and there you have it! The magic unfolds when Pydantic and the instructor library team up. They not only make language models sit up and listen but also ensure the responses are neat and ready for action. This blog? Your backstage pass to a smoother, more efficient NLP show. If you\u2019re a developer or just someone who loves playing with words, this combo is your secret weapon.</p> <p>Please leave your thoughts on this. Thanks</p> <p>Happy Coding!   </p>"},{"location":"writing/2024/03/06/retrieval-augmented-gen/","title":"RAG","text":""},{"location":"writing/2024/03/06/retrieval-augmented-gen/#what-interested-me-in-doing-this","title":"What interested me in doing this...","text":"<p>I was really interested in AI, which is literally the most important tool to enhance everyone's life and solve real-time problems that require significant human resources. By replacing them with AI, we can address all these issues.</p> <p>I became interested in building projects around the APIs of models, GPTs, and OSS models. When I was a beginner, it seemed really cool to generate new content from foundational models. However, when I asked real-time questions or something outside the model's training data, I often received default answers like, \"I'm only trained on data up to 2022.\" </p> <p>This made me question: what are the ways I can train the foundational model on real-time data and make it available for users? This curiosity led me to delve into fine-tuning, which involves training the foundational model on your private data.</p> <p>I know that fine-tuning foundational models requires more capital, which prompted me to explore OSS models. I understood that training OSS models requires more specifications for my laptop.</p> <p>As an enthusiast in solving these problems, I was introduced to RAG.</p>"},{"location":"writing/2024/03/06/retrieval-augmented-gen/#what-is-rag","title":"What is RAG?","text":"<p>RAG stands for Retrieval Augmented Generation. I know it's such a fancy name, and it does fancy stuff too. While understanding this, let's delve into a pictorial understanding of how it's efficient for developers interested in AI and companies seeking to automate their customer support.</p> <p>let's understand the architecture behind the RAG...</p> <p></p>"},{"location":"writing/2024/03/06/retrieval-augmented-gen/#vector-stores","title":"Vector Stores","text":"<p>Okay, imagine you've got this gigantic library, but instead of books lining the shelves, it's filled with these abstract representations of information, called vectors. Each vector is like a unique fingerprint, containing all sorts of details about a piece of data, whether it's a fact, a concept, or something else entirely.</p> <p>Now, what makes these vector stores so cool is that they're organized in a way that makes it super easy for the system to find exactly what it needs. So when you ask a question, it's like the system knows exactly which shelf to pull from to find the answer you're looking for.</p> <p></p>"},{"location":"writing/2024/03/06/retrieval-augmented-gen/#embedding-models","title":"Embedding Models","text":"<p>Think of embedding models as the translators of the AI world. They take all kinds of raw data, like words, sentences, or even images, and convert them into these neat little packages of information called embeddings.</p> <p>These embeddings are like distilled versions of the original data, containing just the essential bits needed to understand what it's all about. So when the system needs to figure out what you're talking about or find related information, it can rely on these embeddings to make sense of things.</p> <p></p>"},{"location":"writing/2024/03/06/retrieval-augmented-gen/#llm","title":"LLM","text":"<p>Now, let's talk about the Language Model (LLM). This is where the magic really happens. It's like having a super-smart friend who's not only great at understanding what you're saying but can also come up with responses that sound totally natural.</p> <p>The LLM is powered by some seriously advanced technology, using all sorts of clever tricks to generate text that's not just accurate but also fits seamlessly into the conversation. So whether you're asking a question or having a chat, the LLM's got your back.</p> <p>RAG brings together these components in a seamless manner, allowing for efficient retrieval and generation of content. By leveraging real-time data and advanced AI techniques, RAG represents a significant leap forward in the field of content generation.</p> <p>this is it for the blog, hope y'all understood how the RAG works, try implementing some stuff around this concept and you'll be familiar with the detials of the architecture. the most used framework for building such applications is LangChain. </p> <p>GitHub Repo for reference: Link</p>"},{"location":"writing/2024/06/01/reduce-llm-costs/","title":"How to reduce almost 80% of LLM costs","text":"<p>If you are an AI developer or building a SAAS wrapped around AI foundational models, its really necessary to think about spending a lot of money on LLMs without any prior knowledge on how to reduce these costs..</p> <p></p>"},{"location":"writing/2024/06/01/reduce-llm-costs/#change-the-model","title":"Change the Model","text":"<p>By replacing the LLM like GPT-4 with a small language models like phi-3 or Mistral for specific tasks that doesn't need more precise and optimized responses. this way you can have major cost cuttings.</p> <p></p>"},{"location":"writing/2024/06/01/reduce-llm-costs/#llm-router","title":"LLM Router","text":"<p>Use different models for different tasks, use the SLMs (Small Language Models) for interacting and intial tasks. If the SLMs couldn't provide a better response for the user prompt, In this case send the user prompt to LLMs to generate a more consice and efficient response.</p> <p>as mentioned in the latest paper ROUTERBENCH: A Benchmark for Multi-LLM Routing System</p> <p></p>"},{"location":"writing/2024/06/01/reduce-llm-costs/#multi-agent-setup","title":"Multi-Agent Setup","text":"<p>As been a contributor to the AutoGen framework (Microsoft), i believe the multi agent setup can solve much more broader use cases than using a single model like GPT4, etc.. </p> <p></p> <ul> <li>Every agent can be powered with a different LLM</li> <li>Can add multiple agents into a groupchat making it much more diverse</li> <li>You can also use the function calling to call you external APIs</li> <li>Making it cost effective</li> </ul> <p>This allows us to only allow agents which need lot of understanding and solve critical problems be backed by the powerful LLMs(GPT4)</p>"},{"location":"writing/2024/06/01/reduce-llm-costs/#llm-lingua","title":"LLM Lingua","text":"<p>LLM Lingua is a method introduced by Microsoft that focuses on optimizing the input and output of large language models. By removing unnecessary tokens and words from the input, you can significantly reduce the cost of running the model. This method is particularly effective for tasks such as summarization or answering specific questions based on a transcript.</p> <p></p> <ul> <li>Reduces the cost on input tokens</li> <li>Uses the small model to compress your prompt and then pass it to the LLM</li> <li>This adds more meaning and value to your prompt</li> </ul> <p>As a Lead GenAI Consultant at VEnableAI, we provide a comprehensive array of services, including AI Agents Workflow and chatbots, designed to optimize performance and maximize results while minimizing costs.</p>"},{"location":"writing/2024/03/17/run-open-source-llms/","title":"How to Make the Best Out of Open-Source LLMs","text":"<p>Ever found yourself wondering if you could tap into the magic of top-notch language models without breaking the bank? Well, good news \u2013 open-source LLMs like Gemma, Mistral, Phi2, and a bunch of others are here to save the day! And guess what? They won't cost you a single penny. If you're scratching your head about how to get them up and running on your own machine and use them for all sorts of cool stuff, you're in the right place. Let's dive in!</p>"},{"location":"writing/2024/03/17/run-open-source-llms/#why-bother-with-open-source-llms","title":"Why Bother with Open-Source LLMs?","text":"<p>Okay, so GPT-4 is like the rockstar of AI models, but let's be real \u2013 it's got a pretty steep price tag attached. And for us students, coughing up a bunch of cash for a project just isn't in the cards. But fear not, my friend, because here's where open-source LLMs come strutting in to save the day. They're like the friendly neighborhood superheroes of the AI world, and here's why they're totally awesome:</p> <ul> <li> <p>They allow you to experience the power of AI without draining your wallet. </p> </li> <li> <p>These models thrive on community contributions. It's like a package, you get everything from model to resources, code and dev support.</p> </li> <li> <p>They don't collect your data, because you are running it in your local machine without use of internet.</p> </li> </ul> <p>These are major reasons to use an open-source LLM instead emptying your pockets. As per my perspective, I think every organization, take any MNC, doesn't use GPT models they train and finetune the open-source LLMs on their data and serve the tasks. Using open-source LLMs would really give you an upperhand from other devs too.</p>"},{"location":"writing/2024/03/17/run-open-source-llms/#introducing-lm-studio","title":"Introducing LM Studio","text":"<p>The beast, helping out fellows to run LLMs on their laptop, entirely offline. Most exciting reason is, LM Studio allows you to download HuggingFace models too. I really cant stop using it for my projects, this really cut off my spendings on text generation. Lets delve into this:</p>"},{"location":"writing/2024/03/17/run-open-source-llms/#how-to-install-lm-studio","title":"How to Install LM Studio","text":"<p>Get to their official website: LM Studio</p> Tip <p>It works on all three machines  Mac, Windows, Linux</p> <p></p> <p>Install the app according to your machine. Once the LM Studio is installed, start the application and will find an interface something more like this..</p> <p></p>"},{"location":"writing/2024/03/17/run-open-source-llms/#lets-explore-each-section","title":"Lets Explore each Section","text":"<p>The above image shows the options to download the model into your local machine using LM Studio.. I downloaded phi-2, a small language model which is really good. Lets see how to run the model and use it using openai</p> <p></p> <p>Start the server, which will run your downloaded LLM in you local machine and you can start using it with the below options..</p> python <pre><code>from openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:123/v1\", api_key=\"not-needed\")\n\ncompletion = client.chat.completions.create(\nmodel=\"local-model\", # this field is currently unused\nmessages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n],\ntemperature=0.7,\n)\n\nprint(completion.choices[0].message)\n</code></pre> curl <pre><code>curl http://localhost:123/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{ \n\"messages\": [ \n    { \"role\": \"system\", \"content\": \"Always answer in rhymes.\" },\n    { \"role\": \"user\", \"content\": \"Introduce yourself.\" }\n], \n\"temperature\": 0.7, \n\"max_tokens\": -1,\n\"stream\": false\n}'\n</code></pre> <p></p> <p></p> <p>LM Studio gives you an exciting feature to see the logs of streaming the tokens of the LLM while generating. this is really awesome!! It also provides an interface like chatgpt to start a converstation and talk to your downloaded LLM, this doesn't require starting the server.</p> <p></p> <p>This concludes our guide! Thank you for joining me on this journey. I hope this blog has made it easier for you to download open-source LLMs onto your local machine. I'm genuinely excited to see what incredible applications you'll create with these powerful tools. Your creativity and innovation are what make this community so vibrant. Keep building, keep exploring, and let's continue to push the boundaries of what's possible together!</p> <p>Feel free to drop your suggestions at discussions.</p>"},{"location":"writing/2024/03/09/ssh-secure-shell/","title":"SSH - Secure Shell","text":""},{"location":"writing/2024/03/09/ssh-secure-shell/#introduction","title":"Introduction","text":"<p>Secure Shell, is a cryptographic network protocol that allows secure communication between two systems over an unsecured network. It provides a secure channel for data transmission, remote login, and other network services. SSH operates on the client-server model, where the SSH server runs on the remote system, and the SSH client is used to access it from a local system.</p> <p>AAAhhh its just the typical definition again. lets understand the concept of SSH to get this definition into our head..</p>"},{"location":"writing/2024/03/09/ssh-secure-shell/#getting-cozy-with-ssh","title":"Getting Cozy with SSH","text":"<p>Ever wondered how we ensure our data stays safe while transfering our data from your machine to other through the internet, i know it passes through many lanes to reach the other machine. there's a catch here, anyone in the network can access your data before reaching the reciever machine and can manipulate the data. </p> <p>alright lets picturize this..</p> <p></p> <p>If you think your data is safe and cannot be accessed then you are wrong, anyone connected to the network can access your data. and that's where the magic of SSH comes into play..</p>"},{"location":"writing/2024/03/09/ssh-secure-shell/#secret-tunnel","title":"Secret Tunnel","text":"<p>SSH creates a secure tunnel between our machine and the one we're communicating with. This tunnel is like a VIP lane in a city with huge traffic, sounds interesting right! it bypasses all the chaos and keeps our data shielded from unwanted attention.</p> <p>This is all fine but how does the SSH creates a secret tunnel in this huge internet networks?</p>"},{"location":"writing/2024/03/09/ssh-secure-shell/#triple-handshake","title":"Triple Handshake","text":"<p>SSH doesn't just reply on one layer protection, it employs a triple layer of protection to keep our data safe from the internet.</p> <ul> <li>Operates ad the Client - Server Model to establish a secure connection.</li> </ul>"},{"location":"writing/2024/03/09/ssh-secure-shell/#ssh-keys","title":"SSH Keys","text":"<p>Firstly we need the ssh keys, but where do we get them. simple open your terminal and search for .ssh directory</p> <ul> <li>Use the below command shown in the image to check if the .ssh file exists with the public and private ssh keys for your local machine</li> </ul> <p></p> Note <p>If not found, use command <code>ssh-keygen</code> to generate the public and private key</p> <p>you can check the public and private keys using  <code>cat</code> command.</p> <p></p>"},{"location":"writing/2024/03/09/ssh-secure-shell/#client-server-connection","title":"Client-Server Connection","text":"<p>We generated the SSH keys already, now the steps of connection starts, as shown in the above figure we'll delve into each step understanding it clearly</p> <ol> <li> <p>Client sends its public key, as in you send your generated public ssh key to the server to store the key.</p> </li> <li> <p>Server sends a decrypted randomly generated string to the client over the network.</p> </li> <li> <p>Client uses its private key to encrypt the string and the encrypted data is sent back to the server.</p> </li> <li> <p>Server then uses the shared public ssh key and decrypts the data send by the client.</p> <ul> <li>if the decrypted data matches the randomly generated string in the step 1, the server makes a connection with the client and creates a secret tunnel. </li> <li>then adds the ssh key to the authorised keys directory in the server to authorize the client.</li> </ul> </li> </ol> <p>Here you go the connection is established and a secret tunnel is created. YAYYYY!</p>"},{"location":"writing/2024/03/09/ssh-secure-shell/#wrapping-up","title":"Wrapping Up","text":"<p>And there you have it \u2013 the inner workings of SSH demystified! By employing encryption, authentication, and a triple handshake, SSH ensures our data remains safe and sound as it traverses the vast network of the internet.</p> <p>So the next time you're zipping your data across the web, remember \u2013 SSH has got your back, keeping your digital conversations private and secure.</p> <p>Stay safe, stay secure, and keep SSHing like a pro!</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/","title":"Here\u2019s why: To understand the layers in the best way!","text":"<p>As a former data analyst enthusiast, I have always recognized the value that visualizations bring to understanding complex data. The Visuals have the power to uncover patterns, reveal insights, and communicate information in a way that mere numbers and statistics cannot.</p> <p>However, my fascination with visualization didn\u2019t stop at data alone. As I dived into the world of deep learning and neural networks, I realized the critical importance of being able to visualize the complex architectures and inner workings of these complex models.</p> <p>Fortunately, Visualkeras comes to the rescue as a powerful tool that simplifies the process of visualizing deep learning models. In this blog post, we\u2019ll delve into the world of Visualkeras and explore how it enables us to unlock the hidden secrets within our neural networks.</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#the-architecture-of-a-neural-network","title":"the Architecture of a Neural Network","text":""},{"location":"writing/2023/07/05/visualize-deep-learning-model/#the-main-components-of-a-neural-network-are","title":"The main components of a neural network are","text":"<p>Input \u2014 The input is a measure of the feature of the model. In simple words, input is the set of attributes fed into the model for learning purposes.</p> <p>Weights \u2014 Weights are similar to scalar multiplication. The primary purpose of weights in a neural network is to emphasize the attributes that contribute the most to the learning process. It is achieved by applying scalar multiplication to the input value and weight matrix. We can understand the importance of each input and the directionality from the respective weights.</p> <p>Transfer function \u2014 The Transfer function is different from the other components because it takes multiple inputs. The transfer function combines several inputs into one output value so that the activation function can be applied.</p> <p>Activation Function \u2014 An Activation function will transform the number from the transfer function into a value that represents the input. Most of the time, the activation function will be non-linear. Without it, the output would be a linear mixture of the input values, with no ability to incorporate non-linearity into the network. </p> <p>Two common activation functions are \u2014 ReLu and Sigmoid.</p> <p>Bias \u2014 The purpose of bias is to change the value produced by the activation function.</p> <p>An artificial neural network comprises three layers \u2014 input, output, and one or more hidden layers. Each layer consists of several neurons stacked in a row. Similarly, a multi-layer neural network consists of many layers arranged next to each other.</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#understanding-the-need-for-visualization","title":"Understanding the Need for Visualization","text":"<p>Before diving into Visualkeras, let\u2019s briefly discuss why visualization is crucial in the context of deep learning models. Deep learning architectures can consist of numerous layers and connections, making it challenging to comprehend their inner workings. Visualizing these models helps us gain a high-level understanding of their structure, identify patterns, and pinpoint potential issues such as overfitting or underfitting.</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#introducing-visualkeras","title":"Introducing Visualkeras","text":"<p>Visualkeras is a Python library that simplifies the visualization of deep learning models using a unified and intuitive interface. It builds upon the popular Keras library and provides an interactive way to visualize the model\u2019s architecture, making it easier to analyze and communicate complex network designs.</p> <pre><code>! pip install visualkeras\n</code></pre>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#visualizing-model-architecture","title":"Visualizing Model Architecture:","text":"<p>With Visualkeras, we can generate comprehensive visualizations of our deep learning models. This includes visual representations of various layer types, such as convolutional layers, recurrent layers, dense layers, and more. By plotting these layers, we can observe the flow of information within the network, visualize parameter shapes, and identify potential bottlenecks or areas for improvement.</p> <p>For more understanding let's build a simple model with 3 Dense layers</p> <pre><code>import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n</code></pre>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#to-visualize-the-above-model","title":"To visualize the above model","text":"<pre><code>import visualkeras\nvisualkeras.layered_view(model)\n</code></pre>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#the-visualization-of-the-3-layers","title":"The visualization of the 3 layers","text":"<p>Visualkeras allows us to export the visualizations in various formats, including image files (PNG, JPEG) and interactive HTML files. These export options enable us to seamlessly integrate visualizations into presentations, reports, or documentation, making it easier to convey our findings and share insights with others.</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#saving-the-visualization-in-png","title":"saving the visualization in .png","text":"<pre><code>visualkeras.layered_view(model).save('model_visualization.png')\n</code></pre>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#saving-the-visualization-as-interactive-html","title":"saving the visualization as interactive html","text":"<pre><code>visualkeras.layered_view(model).to_html('model_visualization.html')\n</code></pre>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#practical-use-cases","title":"Practical Use Cases:","text":"<p>Visualkeras finds application in a wide range of scenarios. Whether you are working on image classification, natural language processing, or time series analysis, visualizing your deep learning models with Visualkeras can provide invaluable assistance in model interpretation, debugging, and fine-tuning. Additionally, it serves as a valuable educational tool for teaching deep learning concepts to students and enthusiasts.</p>"},{"location":"writing/2023/07/05/visualize-deep-learning-model/#conclusion","title":"Conclusion","text":"<p>We learn and understand better when we see the process with our eyes.</p> <p>\u201cThe true understanding of a concept is attained when one observes the intricate workings of its process.\u201d</p> <p>So, why not give Visualkeras a try and unlock the hidden secrets of your neural networks?</p> <p>Good luck with your Data Science journey.</p>"},{"location":"writing/archive/2024/","title":"2024","text":""},{"location":"writing/archive/2023/","title":"2023","text":""},{"location":"writing/category/interview/","title":"Interview","text":""},{"location":"writing/category/gitmatchin/","title":"gitmatch.in","text":""},{"location":"writing/category/fastapi/","title":"FastAPI","text":""},{"location":"writing/category/aws/","title":"AWS","text":""},{"location":"writing/category/tokenizer/","title":"Tokenizer","text":""},{"location":"writing/category/llms/","title":"LLMs","text":""},{"location":"writing/category/ai/","title":"AI","text":""},{"location":"writing/category/cost-managememnt/","title":"Cost Managememnt","text":""},{"location":"writing/category/open-source/","title":"Open-Source","text":""},{"location":"writing/category/computer-networks/","title":"Computer Networks","text":""},{"location":"writing/category/network-protocol/","title":"Network Protocol","text":""},{"location":"writing/category/rag/","title":"RAG","text":""},{"location":"writing/category/automation/","title":"Automation","text":""},{"location":"writing/category/github-actions/","title":"Github Actions","text":""},{"location":"writing/category/mkdocs/","title":"mkdocs","text":""},{"location":"writing/category/docker/","title":"Docker","text":""},{"location":"writing/category/web-developement/","title":"Web Developement","text":""},{"location":"writing/category/deep-learning/","title":"Deep Learning","text":""},{"location":"writing/category/analysis/","title":"Analysis","text":""},{"location":"writing/page/2/","title":"whats up?","text":""},{"location":"writing/archive/2024/page/2/","title":"2024","text":""}]}